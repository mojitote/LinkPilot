# LinkedIn Scraper API - Project Structure

## ğŸ“ Directory Structure

```
linkedin-scraper-api/
â”œâ”€â”€ main.py                 # FastAPI application entry point
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ start.sh               # Quick start script
â”œâ”€â”€ .gitignore             # Git ignore rules
â”œâ”€â”€ README.md              # Project documentation
â”œâ”€â”€ PROJECT_STRUCTURE.md   # This file
â”œâ”€â”€ config.py              # Configuration settings
â”œâ”€â”€ settings.py            # Environment variables
â”œâ”€â”€ Dockerfile             # Docker container definition
â”œâ”€â”€ docker-compose.yml     # Docker Compose configuration
â”œâ”€â”€ services/              # Core scraping services
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ candidate_scraper.py    # Profile scraping logic
â”‚   â”œâ”€â”€ company_scraper.py      # Company scraping logic
â”‚   â””â”€â”€ scraping_utils.py       # Shared utilities and XPath functions
â””â”€â”€ test/                  # Testing and debugging
    â”œâ”€â”€ debug.py           # Manual testing script
    â”œâ”€â”€ htmls/             # Saved HTML files (gitignored)
    â”‚   â””â”€â”€ js/            # JSON results (gitignored)
    â””â”€â”€ test/              # Additional test files
```

## ğŸ”§ Core Components

### `main.py`
- FastAPI application with RESTful endpoints
- Request/response models using Pydantic
- Error handling and validation
- CORS middleware configuration
- Health check endpoints

### `services/candidate_scraper.py`
- Main profile scraping function
- Selenium WebDriver setup
- Cookie injection for authentication
- Page scrolling and "Show more" clicking
- HTML snapshot saving (debug mode)

### `services/scraping_utils.py`
- XPath selectors for data extraction
- Name, headline, avatar extraction
- Education and experience parsing
- About section text extraction
- Chrome WebDriver configuration

### `services/company_scraper.py`
- Company profile scraping logic
- Company-specific data extraction

## ğŸš€ Quick Start

1. **Clone the repository**
2. **Run the quick start script**: `./start.sh`
3. **Access the API docs**: http://localhost:8000/docs
4. **Test with curl**:
   ```bash
   curl -X POST "http://localhost:8000/scrape" \
     -H "Content-Type: application/json" \
     -d '{"url": "https://linkedin.com/in/username", "type": "profile"}'
   ```

## ğŸ” Debugging

Use `test/debug.py` for manual testing:
```bash
cd test
python debug.py
# Enter LinkedIn ID when prompted
```

## ğŸ“Š API Response Format

### Profile Response
```json
{
  "linkedin_id": "username",
  "name": "John Doe",
  "avatar_url": "https://...",
  "headline": "Software Engineer",
  "about": "About text...",
  "education": {
    "positions": ["Bachelor of Science"],
    "institutions": ["University"],
    "dates": ["2018-2022"]
  },
  "experience": {
    "positions": ["Software Engineer"],
    "institutions": ["Tech Corp"],
    "dates": ["2022-Present"]
  },

  "scraped_at": "2024-01-15T10:30:00Z"
}
```

## ğŸ› ï¸ Development

### Adding New Features
1. Add XPath selectors in `scraping_utils.py`
2. Update response models in `main.py`
3. Test with `debug.py`
4. Update documentation

### Environment Variables
- `LINKEDIN_ACCESS_TOKEN`: LinkedIn authentication token
- `HEADLESS`: Run browser in headless mode
- `CHROME_BINARY_PATH`: Custom Chrome path

## ğŸ”’ Security Notes
- LinkedIn cookies required for scraping
- Rate limiting recommended
- CORS configuration for production
- Environment variables for sensitive data 